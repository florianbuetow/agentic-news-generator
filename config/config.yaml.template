# Project directories
paths:
  data_dir: ./data/
  data_models_dir: ./data/models/
  data_downloads_dir: ./data/downloads
  data_downloads_videos_dir: ./data/downloads/videos/
  data_downloads_transcripts_dir: ./data/downloads/transcripts
  data_downloads_transcripts_hallucinations_dir: ./data/downloads/transcripts-hallucinations
  data_downloads_transcripts_cleaned_dir: ./data/downloads/transcripts_cleaned
  data_transcripts_topics_dir: ./data/downloads/transcripts-topics
  data_downloads_audio_dir: ./data/downloads/audio
  data_downloads_metadata_dir: ./data/downloads/metadata
  data_output_dir: ./data/output/
  data_input_dir: ./data/input/
  data_temp_dir: ./data/temp
  data_archive_dir: ./data/archive
  data_archive_videos_dir: ./data/archive/videos
  data_logs_dir: ./logs
  data_output_articles_dir: ./data/output/articles
  data_articles_input_dir: ./data/articles/input

  # Model benchmark and visualization paths
  reports_dir: reports

channels:
  # Example 1: English channel
  - url: https://www.youtube.com/@ExampleChannel1
    name: Example Channel One
    category: example_category
    description: "Description of what content this channel provides."
    download-limiter: 20  # 0=skip downloads, -1=unlimited (99999), >0=max videos to download
    language: "en"  # Source language: English (transcribed in English)

  # Example 2: German channel (will be translated to English)
  - url: https://www.youtube.com/@ExampleChannel2
    name: Example Channel Two
    category: another_example_category
    description: "Another description of channel content and value proposition."
    download-limiter: -1  # Unlimited downloads
    language: "de"  # Source language: German (translated to English)

  # Example 3: Japanese channel (will be translated to English)
  - name: Example Channel Three
    url: https://www.youtube.com/@ExampleChannel3
    category: japanese_tech_channel
    description: "Japanese-language channel about technology."
    download-limiter: 0  # Skip this channel entirely
    language: "ja"  # Source language: Japanese (translated to English)

# Hallucination Detection Configuration
hallucination_detection:
  min_window_size: 500  # Minimum sliding window size in words
  overlap_percent: 25.0  # Overlap percentage between windows (0-100)

  # Output directory for hallucination detection results (relative to data_dir)
  output_dir: downloads/transcripts-hallucinations

  # SVM model coefficients (auto-generated from training)
  coef_repetitions: 0.8888460000
  coef_sequence_length: 0.6665380000
  intercept: -6.7770510000

# Transcription Configuration (MLX Whisper)
transcription:
  # Whisper models
  model_en_name: "medium.en"  # English-only model name
  model_en_repo: "mlx-community/whisper-medium.en-mlx"  # English-only model repository
  model_multi_name: "medium"  # Multilingual model name
  model_multi_repo: "mlx-community/whisper-medium-mlx"  # Multilingual model repository

  # Anti-hallucination settings
  hallucination_silence_threshold: 2.0  # Seek past silence after N seconds if hallucination detected
  compression_ratio_threshold: 2.0  # Lower = stricter (default 2.4, we use 2.0)

  # Metadata configuration
  use_youtube_metadata: true  # Use video title/description in prompts
  description_max_length: 500  # Maximum characters to include from video description

  # Processing configuration
  sleep_between_files: 5  # Seconds to pause between transcribing files

  # Metadata subdirectory names
  metadata_video_subdir: "video"  # Subdirectory for video metadata (.info.json)

  # Output verbosity
  verbose: false  # Set to true for detailed output

# Topic Segmentation Agent Configuration
topic_segmentation:
  # Segmentation Agent LLM - using LM Studio
  agent_llm:
    model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
    api_base: http://127.0.0.1:1234/v1
    api_key: lm-studio
    context_window: 262144
    max_tokens: 32000
    temperature: 0.7
    context_window_threshold: 90  # Raise error if token usage exceeds 90% of context window
    max_retries: 3
    retry_delay: 2.0
    timeout_seconds: 120

  # Critic Agent LLM - using same LM Studio instance
  critic_llm:
    model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
    api_base: http://127.0.0.1:1234/v1
    api_key: lm-studio
    context_window: 262144
    max_tokens: 32000
    temperature: 0.3
    context_window_threshold: 90  # Raise error if token usage exceeds 90% of context window
    max_retries: 3
    retry_delay: 2.0
    timeout_seconds: 120

  # Retry configuration
  retry_limit: 1

# Article Generation Agent Configuration
article_generation:
  editor:
    editor_max_rounds: 3

    output:
      final_articles_dir: ./data/output/articles
      run_artifacts_dir: ./data/output/article_editor_runs
      save_intermediate_results: true

    prompts:
      root_dir: ./prompts/article_editor
      writer_prompt_file: writer.md
      revision_prompt_file: revision.md
      article_review_prompt_file: article_review.md
      concern_mapping_prompt_file: concern_mapping.md
      specialists_dir: specialists

  agents:
    writer_llm:
      model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
      api_base: http://127.0.0.1:1234/v1
      api_key: lm-studio
      context_window: 262144
      max_tokens: 4096
      temperature: 0.7
      context_window_threshold: 90
      max_retries: 3
      retry_delay: 2.0
      timeout_seconds: 180

    article_review_llm:
      model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
      api_base: http://127.0.0.1:1234/v1
      api_key: lm-studio
      context_window: 262144
      max_tokens: 2048
      temperature: 0.3
      context_window_threshold: 90
      max_retries: 3
      retry_delay: 2.0
      timeout_seconds: 90

    concern_mapping_llm:
      model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
      api_base: http://127.0.0.1:1234/v1
      api_key: lm-studio
      context_window: 262144
      max_tokens: 2048
      temperature: 0.3
      context_window_threshold: 90
      max_retries: 3
      retry_delay: 2.0
      timeout_seconds: 60

    specialists:
      fact_check_llm:
        model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
        api_base: http://127.0.0.1:1234/v1
        api_key: lm-studio
        context_window: 262144
        max_tokens: 2048
        temperature: 0.2
        context_window_threshold: 90
        max_retries: 3
        retry_delay: 2.0
        timeout_seconds: 60

      evidence_finding_llm:
        model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
        api_base: http://127.0.0.1:1234/v1
        api_key: lm-studio
        context_window: 262144
        max_tokens: 2048
        temperature: 0.2
        context_window_threshold: 90
        max_retries: 3
        retry_delay: 2.0
        timeout_seconds: 60

      opinion_llm:
        model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
        api_base: http://127.0.0.1:1234/v1
        api_key: lm-studio
        context_window: 262144
        max_tokens: 2048
        temperature: 0.3
        context_window_threshold: 90
        max_retries: 3
        retry_delay: 2.0
        timeout_seconds: 60

      attribution_llm:
        model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
        api_base: http://127.0.0.1:1234/v1
        api_key: lm-studio
        context_window: 262144
        max_tokens: 2048
        temperature: 0.2
        context_window_threshold: 90
        max_retries: 3
        retry_delay: 2.0
        timeout_seconds: 60

      style_review_llm:
        model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
        api_base: http://127.0.0.1:1234/v1
        api_key: lm-studio
        context_window: 262144
        max_tokens: 2048
        temperature: 0.3
        context_window_threshold: 90
        max_retries: 3
        retry_delay: 2.0
        timeout_seconds: 60

  knowledge_base:
    data_dir: ./data/knowledgebase
    index_dir: ./data/knowledgebase_index
    chunk_size_tokens: 512
    chunk_overlap_tokens: 50
    timeout_seconds: 30
    embedding:
      provider: lmstudio
      model_name: text-embedding-bge-large-en-v1.5
      api_base: http://127.0.0.1:1234/v1
      api_key: lm-studio
      timeout_seconds: 30

  perplexity:
    api_base: https://api.perplexity.ai
    api_key: "<REQUIRED_NON_EMPTY_STRING>"
    model: sonar
    timeout_seconds: 45

  institutional_memory:
    data_dir: ./data/institutional_memory
    fact_checking_subdir: fact_checking
    evidence_finding_subdir: evidence_finding

  allowed_styles:
    - "NATURE_NEWS"
    - "SCIAM_MAGAZINE"

  default_style_mode: "SCIAM_MAGAZINE"

# Topic Detection Configuration (Embedding-based segmentation + LLM topic extraction)
topic_detection:
  # Embedding generator configuration
  embedding:
    provider: lmstudio                  # Provider type (factory selector)
    model_name: text-embedding-bge-large-en-v1.5
    api_base: http://127.0.0.1:1234/v1
    api_key: lm-studio                  # For LM Studio, any non-empty value works

  # Sliding window segmenter settings
  sliding_window:
    window_size: 100         # Words per chunk for embedding
    stride: 5                # Words to advance between chunks (95% overlap)
    threshold_method: relative
    threshold_value: 0.4
    smoothing_passes: 1

  # Topic detection agent LLM
  topic_detection_llm:
    model: openai/qwen3-30b-a3b-thinking-2507-mlx@8bit
    api_base: http://127.0.0.1:1234/v1
    api_key: lm-studio
    context_window: 262144
    max_tokens: 4096
    temperature: 0.3
    context_window_threshold: 90
    max_retries: 3
    retry_delay: 2.0
    timeout_seconds: 120

  # Output settings
  output_dir: output/topics

# Default parameter values
defaults:
  # Token validation defaults
  encoding_name: "o200k_base"  # Tiktoken encoding for token counting

  # Repetition detection defaults
  repetition_min_k: 1  # Minimum phrase length for detection
  repetition_min_repetitions: 5  # Minimum consecutive repetitions
  detect_min_k: 3  # Default min_k for detect() method

# Article Compiler Configuration
article_compiler:
  input_dir: "data/input/newspaper/articles"
  output_file: "data/input/newspaper/articles.js"
  min_articles: 21
  date_format: "%Y-%m-%d"

  paragraphs:
    hero_count: 2
    secondary_count: 2
    featured_count: 1

  images:
    extract_first: true
    fallback_url: null

  links:
    base_path: "/articles/"
    slug_from_filename: true
